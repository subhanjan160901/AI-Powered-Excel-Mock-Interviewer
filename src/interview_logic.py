import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from .perplexity_detector import is_ai_generated

llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7, top_p=0.9)

# Pre-defined list of Excel questions for the interview
EXCEL_QUESTIONS = [
    "What is the difference between the VLOOKUP and HLOOKUP functions in Excel?",
    "Explain how to use the INDEX and MATCH functions together, and why you might prefer them over VLOOKUP.",
    "Describe what a Pivot Table is and give an example of a scenario where it would be useful.",
    "What is Conditional Formatting in Excel? Can you provide an example?",
    "How would you use the IFERROR function? Give a practical example.",
    "Explain the purpose of the SUMIFS function and how it differs from SUMIF."
]

def start_interview(state: dict) -> dict:
    """
    Node to start the interview, introduce the agent, and set up the initial state.
    """
    intro_message = (
        "Welcome to the automated Excel skills assessment. This system is designed to help us understand your proficiency in advanced Excel, a key skill for our Finance, Operations, and Data Analytics roles. "
        "I will ask you a series of technical questions. Please provide clear and concise answers. Your performance summary will be generated at the end. "
        "Let's begin."
    )
    # Initialize all necessary keys in the state
    return {
        **state,
        "interview_status": 1,  # 1 = In Progress
        "interview_history": [("ai", intro_message)],
        "questions": EXCEL_QUESTIONS,
        "question_index": 0,
        "evaluations": [],
        "warnings": [],
        "final_feedback": "",
    }

def ask_question(state: dict) -> dict:
    """
    Node to get the next question from the list and add it to the history.
    """
    question_index = state["question_index"]
    current_question = state["questions"][question_index]
    
    history = state.get("interview_history", [])
    history.append(("ai", current_question))
    
    return { **state, "interview_history": history }

# --- MODIFIED FUNCTION ---
def process_user_response(state: dict) -> dict:
    """
    Checks for AI generation. If found, terminates the interview. 
    Otherwise, evaluates the answer for correctness.
    """
    history = state.get("interview_history", [])
    user_response = history[-1][1]
    
    # --- Step 1: Check for AI-generated text ---
    if is_ai_generated(user_response, threshold=45.0):
        print("AI: (AI-generated text detected. Terminating interview.)")
        termination_message = (
            "This interview will now be terminated. Our system has detected that the answer provided was "
            "likely generated by an AI tool, which is not permitted for this assessment."
        )
        # Add the termination message to the history
        history.append(("ai", termination_message))
        
        # Update the state to end the interview
        return {
            **state,
            "interview_history": history,
            "interview_status": 2,  # 2 = Finished/Terminated
        }
    
    # --- Step 2: If not AI-generated, proceed with normal evaluation ---
    print("AI: (Answer appears human-written. Evaluating for correctness...)")
    current_question = state["questions"][state["question_index"]]

    evaluation_prompt = ChatPromptTemplate.from_messages([
        # ... (evaluation prompt is the same) ...
        ("system", "You are an automated evaluation system..."),
        ("human", f"Question: {current_question}\n\nCandidate's Answer: {user_response}")
    ])

    evaluator_chain = evaluation_prompt | llm
    evaluation = evaluator_chain.invoke({}).content
    
    evaluations = state.get("evaluations", [])
    evaluations.append(evaluation)

    return {
        **state,
        "evaluations": evaluations,
        "question_index": state["question_index"] + 1,
        # We no longer need to manage warnings here, as it's now a termination event
        "warnings": [] 
    }

def generate_final_report(state: dict) -> dict:
    """
    Node to generate the final performance summary at the end of the interview.
    """
    print("AI: (Generating final report...)")
    interview_transcript = "\n".join([f"{speaker.capitalize()}: {text}" for speaker, text in state['interview_history']])
    evaluations_summary = "\n\n".join(f"Evaluation for Question {i+1}:\n{e}" for i, e in enumerate(state['evaluations']))

    report_prompt = ChatPromptTemplate.from_template(
        "You are an AI Recruitment Analyst. Your mission is to generate a comprehensive, constructive performance summary for a candidate who has just completed a mock technical interview for an advanced Excel role (e.g., for Finance, Operations, or Data Analytics).\n\n"
        "You will be given the full interview transcript and a series of per-question evaluations. Your report must be structured using Markdown as follows:\n\n"
        "### Overall Performance Summary\n"
        "A brief, high-level summary of the candidate's performance, mentioning their overall grasp of Excel concepts.\n\n"
        "### Strengths\n"
        "- Use bullet points to list specific areas where the candidate excelled. Mention specific examples from the interview (e.g., 'Provided a clear and accurate explanation of INDEX/MATCH').\n\n"
        "### Areas for Improvement\n"
        "- Use bullet points to list specific, actionable areas for improvement. Be constructive and suggest what to focus on (e.g., 'Could provide more detail on the practical business use cases for Pivot Tables.').\n\n"
        "The tone should be professional, encouraging, and objective, providing clear evidence from the interview to support your points.\n\n"
        "---\n**Interview Transcript**\n{transcript}\n\n"
        "---\n**Answer Evaluations**\n{evaluations}\n---"
    )
    report_chain = report_prompt | llm
    final_feedback = report_chain.invoke({"transcript": interview_transcript, "evaluations": evaluations_summary}).content
    
    history = state.get("interview_history", [])
    history.append(("ai", final_feedback))

    return {**state, "final_feedback": final_feedback, "interview_history": history, "interview_status": 2} # 2 = Finished


# --- Routing Logic for the Graph ---

# --- MODIFIED ROUTER FUNCTION ---
def route_after_evaluation(state: dict) -> str:
    """
    Router to decide the next step after processing a response.
    """
    # First, check if the interview was just terminated by the AI detection logic.
    if state.get("interview_status") == 2:
        # The interview is over, so we go directly to the end.
        return "terminate"
        
    # If not terminated, check if we've run out of questions normally.
    elif state["question_index"] >= len(state["questions"]):
        return "generate_final_report"
        
    # Otherwise, continue to the next question.
    else:
        return "ask_question"

def route_start_of_interview(state: dict):
    """
    Router for the entry point of the graph. Decides whether to start a new interview
    or process the user's response from an ongoing one.
    """
    if state.get("interview_status", 0) == 0:
        return "start_interview"
    else:
        return "process_user_response"